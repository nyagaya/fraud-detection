{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals and Objectives: \n",
    "Implement a couple of machine learning techniques that accurately predicts fraudulent credit card transactions. In addition to the machine learning applications, explore the data via EDA to gain a better understanding of features, although this maybe limited considering all numeric features were reduced to principal components of the original variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description: https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "This is a current Kaggle data set where the task is to predict fraud. It contains credit card transactions made in 2013 by European card holders, recorded over a two-day period. It has about 285 thousand observations and 31 features. In total, there are only 492 fraudulent transactions (0.17%) and 285 thousand non-fraud transactions. For privacy reasons, all numeric features were reduced to lower dimension before distribution, using PCA into 28 principal components V1 through V28. For similar reasons, no additional feature descriptions were provided. Other variables were Time (time elapsed after the first transaction) and Amount (charged transaction) as well as the class label where 1 represented fraud event and 0 otherwise. A major limitation in the data was the severe class-imbalance which represented a significant challenge in correctly classifying fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "After preprocessng the data, the techniques attempted are a logistic regression and a random forest. Thereafter, we balanced the data using techniques presented in section 3 - data balancing. With class imbalance, correct measures of accuracy are recall, precsion, and F1-score. We noted that both two algorithms did not peform well with the imbalanced data, though the random forest was materially better compared to the logistic regression. After employing resampling techniques - undersampling - we hyperparameter tuned a new random forest on the balanced undersampled data, then using the best estimators, fit a FINAL random forest where we achieved robust recall values.\n",
    "\n",
    "* Logistic Regression: the choice of the logistic regression was more about explainability rather than accuracy. However as will see below, this data set may have been too complex for a simple logistic regression thus the lower performance of the model. \n",
    "\n",
    "* Random forest:  as noted under section 3 - balancing data - one of the techniques to handling class imbalance is to employ ensemble techniques. Partly, this was the reason for the choice of the random forest. However, when trained on the class imbalanced data and validated on the unseen class imbalanced test, the random forest, while providing modest improvement over the logistic regression, it did not provide optimal results. Therefore, for the final model, we again chose to hyperparameter tune a random forest, trained on class balanced data, and validated on class imbalanced unseen test set. As shown below, this final model peformed exceedingly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load initial libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\cygwin64\\\\home\\\\Collins Nyagaya'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Collins Nyagaya\\Documents\\Python Scripts\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Collins Nyagaya\\Documents\\Python Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data and visualize header\n",
    "# v1 - v28 appear to beon same scale, from feature \"amount\", scaling maybe needed\n",
    "credit = pd.read_csv(\"creditcard.csv\", na_values=[\"?\"])\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature \"time\" is the number of seconds elapsed between each transaction and the first transaction in the dataset. Feature 'Amount' is the transaction Amount, while feature 'Class' is the target variable and with value 1 = fraud and 0 otherwise. As noted earlier, the other variables v1-v28 are principal components of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining the shape - 31 variables in total, with close to 285K credit card transactions\n",
    "credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# We see the data has two classes - per data description, 1=fraud, 0 otherwise\n",
    "pd.unique(credit[\"Class\"].values.ravel())\n",
    "# number of classes\n",
    "print(\"Number of classes:\", len(pd.unique(credit[\"Class\"].values.ravel())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the dataset suffers from severe class imbalance. We see that class of interest (fraudulent transactions) is less than 1% - from the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ff0edf7da0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAETCAYAAAC4KPoRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nONDMLBsONDPLxv8DffIJzYyPUTQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ff0f000898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examining the class distribution\n",
    "cr = credit[\"Class\"].value_counts()/credit[\"Class\"].count()\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xlabel('Target/Class')\n",
    "ax1.set_ylabel('Percentage')\n",
    "ax1.set_title(\"Class Distribution\")\n",
    "cr.plot(kind='bar', grid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not fraud transactions percent:  99.82725143693798\n",
      "Fraudulent transactions percent:  0.1727485630620034\n"
     ]
    }
   ],
   "source": [
    "# print percentages of the two classes\n",
    "fraud_count = len(credit[credit[\"Class\"]==1])\n",
    "not_fraud_count = len(credit[credit[\"Class\"]==0])\n",
    "\n",
    "percent_not_fraud = not_fraud_count/(not_fraud_count + fraud_count)\n",
    "fraud_percent = fraud_count/(not_fraud_count + fraud_count)\n",
    "print(\"Not fraud transactions percent: \", percent_not_fraud*100)\n",
    "print(\"Fraudulent transactions percent: \", fraud_percent*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two visuals below are for fraudulent and non-fraudulent transactions. We can infer that fraudulent transactions appear to be all less than 2,500 dollars per transaction, with a significant number of transactions concentrated in the 0 and 500 dollars. Alternatively, there is more of a spread in non_fraudulent transactions. This is an interesting observation in that contrary to expectations, fraudulent transactions tend to be in smaller amounts per this dataset. Why that is the case is probably an issue worth further investigatin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ff0f5aac88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAF1CAYAAABcTxaRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ff0edd2d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fraud = credit[credit[\"Class\"]==1]\n",
    "normal = credit[credit[\"Class\"]==0]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(121)\n",
    "fraud.Amount.plot.hist(title=\"Fraudulent Transactions\")\n",
    "plt.subplot(122)\n",
    "normal.Amount.plot.hist(title=\"Non-Fraudulent Transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review column names\n",
    "credit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "Time      284807 non-null float64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "# .info provides data type for each variable, as well as indicates if there are null values\n",
    "# in the data\n",
    "credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another confirmation there are missing values\n",
    "credit.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though not meaningful, the summary stats indicate that the minimum transaction amount is 0.00 dollars while maximum amount is about 26,000 dollars. The mean amount is 88.00 dollars, and we also do see that 75 percent of all transactions were less than 77.00 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.758743e-12</td>\n",
       "      <td>-8.252298e-13</td>\n",
       "      <td>-9.636929e-13</td>\n",
       "      <td>8.316157e-13</td>\n",
       "      <td>1.591952e-13</td>\n",
       "      <td>4.247354e-13</td>\n",
       "      <td>-3.050180e-13</td>\n",
       "      <td>8.693344e-14</td>\n",
       "      <td>-1.179712e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.406543e-13</td>\n",
       "      <td>-5.713163e-13</td>\n",
       "      <td>-9.725303e-13</td>\n",
       "      <td>1.464139e-12</td>\n",
       "      <td>-6.989087e-13</td>\n",
       "      <td>-5.615260e-13</td>\n",
       "      <td>3.332112e-12</td>\n",
       "      <td>-3.518886e-12</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.758743e-12 -8.252298e-13 -9.636929e-13  8.316157e-13   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   1.591952e-13  4.247354e-13 -3.050180e-13  8.693344e-14 -1.179712e-12   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...       -3.406543e-13 -5.713163e-13 -9.725303e-13  1.464139e-12   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean  -6.989087e-13 -5.615260e-13  3.332112e-12 -3.518886e-12      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# though not meaningful, we get the summary statistics of the data\n",
    "credit.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9   ...         V20       V21       V22       V23  \\\n",
       "0  0.098698  0.363787   ...    0.251412 -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425   ...   -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654   ...    0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024   ...   -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739   ...    0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assigning the predictor variables to object X\n",
    "X = credit[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "                'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "                'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assigning target variable to object y\n",
    "y = credit.Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables except time and amount are essentially principal components, as noted already. Therefore, examining variable distributions does not make sense as we cannot really interpret princinpal component variables. However, we will need to scale the data to bring all features to a common scale. Therefore, we standardize amount variable into a new variable using the sklearn's standard scaler. In addition, we drop \"time\" variable as it is essentially a sequantial variable per transaction, and we also drop the original amount variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount_scl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>-0.338556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>-0.333279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>-0.190107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>0.019392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>-0.338516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10     ...           V20       V21       V22  \\\n",
       "0  0.098698  0.363787  0.090794     ...      0.251412 -0.018307  0.277838   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.069083 -0.225775 -0.638672   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.524980  0.247998  0.771679   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.208038 -0.108300  0.005274   \n",
       "4 -0.270533  0.817739  0.753074     ...      0.408542 -0.009431  0.798278   \n",
       "5  0.260314 -0.568671 -0.371407     ...      0.084968 -0.208254 -0.559825   \n",
       "6  0.081213  0.464960 -0.099254     ...     -0.219633 -0.167716 -0.270710   \n",
       "7 -3.807864  0.615375  1.249376     ...     -0.156742  1.943465 -1.015455   \n",
       "8  0.851084 -0.392048 -0.410430     ...      0.052736 -0.073425 -0.268092   \n",
       "9  0.069539 -0.736727 -0.366846     ...      0.203711 -0.246914 -0.633753   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Amount_scl  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053    0.244964  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   -0.342475  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752    1.160686  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458    0.140534  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   -0.073403  \n",
       "5 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   -0.338556  \n",
       "6 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   -0.333279  \n",
       "7  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   -0.190107  \n",
       "8 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404    0.019392  \n",
       "9 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   -0.338516  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want all variable to be on a common scale. \n",
    "# Standardization, rather than normalization, has been proven to be more effective\n",
    "# as it essentially normalizes the variables to mean 0 and standard deviation 1 \n",
    "# thus achieving a normal distribution.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X[\"Amount_scl\"] = StandardScaler().fit_transform(X[\"Amount\"].values.reshape(-1, 1))\n",
    "X = X.drop([\"Time\", \"Amount\"], axis=1)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with data standardized, we split into train and test set, both X and Y sets\n",
    "# we retain 30% to test the model, using train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor tain set:  (199364, 29)\n",
      "Predictor test set:  (85443, 29)\n",
      "Target train set:  (199364,)\n",
      "Target test set:  (85443,)\n"
     ]
    }
   ],
   "source": [
    "# printing the split data\n",
    "print(\"Predictor tain set: \", X_train.shape)\n",
    "print(\"Predictor test set: \", X_test.shape)\n",
    "print(\"Target train set: \", y_train.shape)\n",
    "print(\"Target test set: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data is clearly class-imbalanced. While there are a number of techniques to dealing with class imbalance issues, we will first build a few models without rebalancing, and assess model quality, afterwhich we will make certain decisions on how to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a function to measure model performance\n",
    "from sklearn import metrics\n",
    "\n",
    "def measure_performance(X, y, clf, show_accuracy=True, show_classification_report=True, show_confussion_matrix=True):\n",
    "    y_pred = clf.predict(X)   \n",
    "    if show_accuracy:\n",
    "         print(\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y, y_pred)),\"\\n\")\n",
    "    if show_classification_report:\n",
    "        print(\"Classification report\")\n",
    "        print(metrics.classification_report(y, y_pred),\"\\n\")\n",
    "      \n",
    "    if show_confussion_matrix:\n",
    "        print(\"Confussion matrix\")\n",
    "        print(metrics.confusion_matrix(y, y_pred),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import logistic function from sklearn\n",
    "# and initialize the logistic reg object\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1.0, random_state=0)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make predictions based on test set\n",
    "lrpred_test = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import classification report\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is highly class imbalanced, the more appropriate measure of correct classification is recall/precsion. From the classification report below, we see that while the logit has average recall/precission of 100% - a perfect classifier -- the classifier is doing a terrible job correctly classifying fraudulent transactions. In effect, recall for the fraud is 62%, meaning the model only labeled 62% of fraudulent transactions as such. Precision is somewhat higher at 88% for the positive class, meaning that 89% of the positive class that were labeled as positive are indeed positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.88      0.62      0.73       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, lrpred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# initialize the confusion matrix\n",
    "lr_cm = (confusion_matrix(y_test, lrpred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAElCAYAAAARL9xtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nItEUGCISTYEhItEUGCIS7f8BGrvViQcpvPkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ff10d90198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the confusion matrix\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "plt.matshow(lr_cm, cmap = plt.cm.Blues)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\\n\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "for y in range(lr_cm.shape[0]):\n",
    "    for x in range(lr_cm.shape[1]):\n",
    "        plt.text(x, y, '{}'.format(lr_cm[y, x]),\n",
    "                horizontalalignment = 'center',\n",
    "                verticalalignment = 'center',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fitting a base random forest on class imbalance data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validated on the class imbalanced test data, the random forest materially improves over the logistic regression. As one would expect, overall accuracy is 100%. Recall on the negative class is 100% as well. However, recall on the positive class is 73%, an 11 percentage improvement over the logistic regression. Interpreted, this means the random forest is accurately predicting 73% of the positve class AS POSITIVE. The F1-score is also strong compared to the logistic regression. \n",
    "\n",
    "#### Question of interest - can the recall be further improved? In the next section, techniques for dealing with class imbalance are identified and employed. We then train a hyperparameter tuned random forest and measure performance on the original unbalanced test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.999 \n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.94      0.72      0.82       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      " \n",
      "\n",
      "Confussion matrix\n",
      "[[85289     7]\n",
      " [   41   106]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# measure performance, on the original unbalanced test set\n",
    "measure_performance(X_test, y_test, rf, show_confussion_matrix=True, show_classification_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the simple logistic regression and random forest, it is clear any algorithm trained on the class-imbalanced data will probably have lower recall values. It is imperative we employ techniques for dealing with class imbalance.  General techniques include:\n",
    "\n",
    "1. Resampling techniques - over/under sampling\n",
    "2. Different performance metrics besides Accuracy: recall/precision, sensitivity, F-measure, AUCPR and Kappa.\n",
    "3. SMOTE - synthetically generate additional features of rare class\n",
    "4. Threshold moving\n",
    "5. Ensemble techniques\n",
    "6. Collect additional data - often expensive, not practical!\n",
    "\n",
    "#### For the modeling tasks included here, we implemented techniques 1 (undersampling), 2, and 5 (ensemble technique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# libraries needed for undersampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({1: 345, 0: 345})\n"
     ]
    }
   ],
   "source": [
    "# initialize the undersampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "# we undersample the original TRAINING data, while leaving the original test set as unseen data\n",
    "X_res, y_res = rus.fit_sample(X_train, y_train)\n",
    "# while faily small, we get a balanced sample that includes 70 percent of fraudulent transactions\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - balanced data\n",
    "With the data balanced, using gridsearch, we fit a hyperparameter tuned random forest on the undersampled data. We then validate model peformance on the original class-imbalance test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize random forest estimator\n",
    "rf_test = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries needed\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 117.68 seconds for 216 candidate parameter settings.\n"
     ]
    }
   ],
   "source": [
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 4, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(rf_test, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X_res, y_res)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': 10,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the best grid search parameters\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the best parameter estimates, we refitted a final model, on the class-balanced data. Wen then measured performance on original class-imbalanced unseen test set. Generally, with a random forest, there is no need for cross validation, as cross validation is performed during the bootstrap aggregation during model training. As can be noted, by hyperparameter tuning, we attained significant improvement to class of interest recall by 20 percentage points to 92%. Essentially, the final model is accurately predicting 92% of the fraudulent transactions as fraud, which is robust in such as highly class imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit the final model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_final = RandomForestClassifier(n_estimators=100, \n",
    "                            min_samples_split=2, \n",
    "                            max_depth=None, \n",
    "                            max_features=10,\n",
    "                           min_samples_leaf=3,\n",
    "                           criterion=\"entropy\",\n",
    "                           bootstrap=False)\n",
    "rf_final = rf_final.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.972 \n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99     85296\n",
      "          1       0.05      0.92      0.10       147\n",
      "\n",
      "avg / total       1.00      0.97      0.98     85443\n",
      " \n",
      "\n",
      "Confussion matrix\n",
      "[[82875  2421]\n",
      " [   12   135]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "measure_performance(X_test, y_test, rf_final, show_confussion_matrix=True, show_classification_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the above analysis, we went through an iterative KDD process, from data understanding, data preprocessing, exploratory data analysis, data reduction/scaling and modeling as well as selecting the final model. As previously noted, considering the severe class imbalance in the data, we employed resampling techniques, different measures of model accuracy as well as ensemble learning. We then trained two models on the class imbalanced data, with modest predictions, afterwhich we balanced the data and trained a final random forest model that achieved significantly better results on the unseen CLASS-IMBALANCED test set. \n",
    "\n",
    "However, it should be noted that while our FINAL model has recall = 92% on the class of interest (overall recall = 98%), precision has materially suffered. However, in fraud detection, my view is that we want to maximize recall on the class of interest at the expense of precision - we care more about predicting that what we predict as fraud, is actually fraud. So while our precision in the final model positive class is negligible, we are okay with that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
